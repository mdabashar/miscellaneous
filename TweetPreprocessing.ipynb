{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet Preprocessing Using ekphrasis\n",
    "ekphrasis Link: https://github.com/cbaziotis/ekphrasis\n",
    "\n",
    "ekphrasis offers the following functionality:\n",
    "\n",
    "1. Social Tokenizer. A text tokenizer geared towards social networks (Facebook, Twitter...), which understands complex emoticons, emojis and other unstructured expressions like dates, times and more.\n",
    "\n",
    "2. Word Segmentation. You can split a long string to its constituent words. Suitable for hashtag segmentation.\n",
    "\n",
    "3. Spell Correction. You can replace a misspelled word, with the most probable candidate word.\n",
    "\n",
    "4. Customization. Taylor the word-segmentation, spell-correction and term identification, to suit your needs.\n",
    "\n",
    "Word Segmentation and Spell Correction mechanisms, operate on top of word statistics, collected from a given corpus. We provide word statistics from 2 big corpora (from Wikipedia and Twitter), but you can also generate word statistics from your own corpus. You may need to do that if you are working with domain-specific texts, like biomedical documents. For example a word describing a technique or a chemical compound may be treated as a misspelled word, using the word statistics from a general purposed corpus.\n",
    "\n",
    "ekphrasis tokenizes the text based on a list of regular expressions. You can easily enable ekphrasis to identify new entities, by simply adding a new entry to the dictionary of regular expressions (ekphrasis/regexes/expressions.txt).\n",
    "\n",
    "5. Pre-Processing Pipeline. You can combine all the above steps in an easy way, in order to prepare the text files in your dataset for some kind of analysis or for machine learning. In addition, to the aforementioned actions, you can perform text normalization, word annotation (labeling) and more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install ekphrasis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Text Pre-Processing pipeline\n",
    "\n",
    "You can easily define a preprocessing pipeline, by using the TextPreProcessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\ekphrasis\\classes\\tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word statistics files not found!\n",
      "Downloading... done!\n",
      "Unpacking... done!\n",
      "Reading twitter - 1grams ...\n",
      "generating cache file for faster loading...\n",
      "reading ngrams C:\\Users\\User\\.ekphrasis\\stats\\twitter\\counts_1grams.txt\n",
      "Reading twitter - 2grams ...\n",
      "generating cache file for faster loading...\n",
      "reading ngrams C:\\Users\\User\\.ekphrasis\\stats\\twitter\\counts_2grams.txt\n",
      "Reading twitter - 1grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\ekphrasis\\classes\\exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    }
   ],
   "source": [
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'url', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "        'emphasis', 'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Some tweets\n",
    "\n",
    "Notes:\n",
    "1. elongated words are automatically normalized.\n",
    "2. Spell correction affects performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"CANT WAIT for the new season of #TwinPeaks ＼(^o^)／!!! #davidlynch #tvseries :)))\",\n",
    "    \"I saw the new #johndoe movie and it suuuuucks!!! WAISTED $10... #badmovies :/\",\n",
    "    \"@SentimentSymp:  can't wait for the Nov 9 #Sentiment talks!  YAAAAAAY !!! :-D http://sentimentsymposium.com/.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<allcaps> cant wait </allcaps> for the new season of <hashtag> twin peaks </hashtag> ＼(^o^)／ ! <repeated> <hashtag> david lynch </hashtag> <hashtag> tv series </hashtag> <happy>\n",
      "\n",
      "i saw the new <hashtag> john doe </hashtag> movie and it sucks <elongated> ! <repeated> <allcaps> waisted </allcaps> <money> . <repeated> <hashtag> bad movies </hashtag> <annoyed>\n",
      "\n",
      "<user> : can not wait for the <date> <hashtag> sentiment </hashtag> talks ! <allcaps> yay <elongated> </allcaps> ! <repeated> <laugh> <url>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in sentences:\n",
    "    print(\" \".join(text_processor.pre_process_doc(s)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Segmentation\n",
    "\n",
    "The word segmentation implementation uses the Viterbi algorithm and is based on CH14 from the book Beautiful Data (Segaran and Hammerbacher, 2009). The implementation requires word statistics in order to identify and separating the words in a string. You can use the word statistics from one of the 2 provided corpora, or from your own corpus.\n",
    "\n",
    "Example: In order to perform word segmentation, first you have to instantiate a segmenter with a given corpus, and then just use the segment() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n",
      "generating cache file for faster loading...\n",
      "reading ngrams C:\\Users\\User\\.ekphrasis\\stats\\english\\counts_1grams.txt\n",
      "Reading english - 2grams ...\n",
      "generating cache file for faster loading...\n",
      "reading ngrams C:\\Users\\User\\.ekphrasis\\stats\\english\\counts_2grams.txt\n",
      "small and insignificant\n"
     ]
    }
   ],
   "source": [
    "from ekphrasis.classes.segmenter import Segmenter\n",
    "seg = Segmenter(corpus=\"english\") \n",
    "print(seg.segment(\"smallandinsignificant\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can test the output using statistics from the different corpora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n",
      "Reading english - 2grams ...\n",
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "exponentialbackoff\n",
      "(eng): exponential backoff\n",
      "(tw): exponential back off\n",
      "\n",
      "gamedev\n",
      "(eng): gamedev\n",
      "(tw): game dev\n",
      "\n",
      "retrogaming\n",
      "(eng): retrogaming\n",
      "(tw): retro gaming\n",
      "\n",
      "thewatercooler\n",
      "(eng): the water cooler\n",
      "(tw): the watercooler\n",
      "\n",
      "panpsychism\n",
      "(eng): panpsychism\n",
      "(tw): pan psych is m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ekphrasis.classes.segmenter import Segmenter\n",
    "\n",
    "# segmenter using the word statistics from english Wikipedia\n",
    "seg_eng = Segmenter(corpus=\"english\") \n",
    "\n",
    "# segmenter using the word statistics from Twitter\n",
    "seg_tw = Segmenter(corpus=\"twitter\")\n",
    "\n",
    "words = [\"exponentialbackoff\", \"gamedev\", \"retrogaming\", \"thewatercooler\", \"panpsychism\"]\n",
    "for w in words:\n",
    "    print(w)\n",
    "    print(\"(eng):\", seg_eng.segment(w))\n",
    "    print(\"(tw):\", seg_tw.segment(w))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, if the word is camelCased or PascalCased, then the algorithm splits the words based on the case of the characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n",
      "Reading english - 2grams ...\n",
      "camel cased\n",
      "pascal cased\n"
     ]
    }
   ],
   "source": [
    "from ekphrasis.classes.segmenter import Segmenter\n",
    "seg = Segmenter() \n",
    "print(seg.segment(\"camelCased\"))\n",
    "print(seg.segment(\"PascalCased\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spell Correction\n",
    "\n",
    "The Spell Corrector is based on Peter Norvig's spell-corrector. Just like the segmentation algorithm, it utilizes word statistics in order to find the most probable candidate. Besides the provided statistics, you can use your own.\n",
    "\n",
    "##### Example:\n",
    "You can perform the spell correction, just like the word segmentation. First you have to instantiate a SpellCorrector object, that uses the statistics from the corpus of your choice and then use on of the available methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n",
      "correct\n"
     ]
    }
   ],
   "source": [
    "from ekphrasis.classes.spellcorrect import SpellCorrector\n",
    "sp = SpellCorrector(corpus=\"english\") \n",
    "print(sp.correct(\"korrect\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Social Tokenizer\n",
    "\n",
    "The difficulty in tokenization is to avoid splitting expressions or words that should be kept intact (as one token). This is more important in texts from social networks, with \"creative\" writing and expressions like emoticons, hashtags and so on. Although there are some tokenizers geared towards Twitter [1],[2], that recognize the Twitter markup and some basic sentiment expressions or simple emoticons, this tokenizer is able to identify almost all emoticons, emojis and many complex expressions.\n",
    "\n",
    "Especially for tasks such as sentiment analysis, there are many expressions that play a decisive role in identifying the sentiment expressed in text. Expressions like these are:\n",
    "1. Censored words, such as f**k, s**t.\n",
    "2. Words with emphasis, such as a *great* time, I don't *think* I ....\n",
    "3. Emoticons, such as >:(, :)), \\o/.\n",
    "4. Dash-separated words, such as over-consumption, anti-american, mind-blowing.\n",
    "\n",
    "Moreover, ekphrasis can identify information-bearing expressions. Depending on the task, you may want to keep preserve / extract them as one token (IR) and then normalize them since this information may be irrelevant for the task (sentiment analysis). Expressions like these are:\n",
    "\n",
    "1. Dates, such as Feb 18th, December 2, 2016, December 2-2016, 10/17/94, 3 December 2016, April 25, 1995, 11.15.16, November 24th 2016, January 21st.\n",
    "2. Times, such as 5:45pm, 11:36 AM, 2:45 pm, 5:30.\n",
    "3. Currencies, such as $220M, $2B, $65.000, €10, $50K.\n",
    "4. Phone numbers.\n",
    "5. URLs, such as http://www.cs.unipi.gr, https://t.co/Wfw5Z1iSEt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ORG:  CANT WAIT for the new season of #TwinPeaks ＼(^o^)／ yaaaay!!! #davidlynch #tvseries :)))\n",
      "\n",
      "WSP :  ['CANT', 'WAIT', 'for', 'the', 'new', 'season', 'of', '#TwinPeaks', '＼(^o^)／', 'yaaaay!!!', '#davidlynch', '#tvseries', ':)))']\n",
      "\n",
      "WPU :  ['CANT', 'WAIT', 'for', 'the', 'new', 'season', 'of', '#', 'TwinPeaks', '＼(^', 'o', '^)／', 'yaaaay', '!!!', '#', 'davidlynch', '#', 'tvseries', ':)))']\n",
      "\n",
      "SC :  ['CANT', 'WAIT', 'for', 'the', 'new', 'season', 'of', '#TwinPeaks', '＼(^o^)／', 'yaaaay', '!', '!', '!', '#davidlynch', '#tvseries', ':)))']\n",
      "=====================\n",
      "\n",
      "ORG:  I saw the new #johndoe movie and it suuuuucks!!! WAISTED $10... #badmovies >3:/\n",
      "\n",
      "WSP :  ['I', 'saw', 'the', 'new', '#johndoe', 'movie', 'and', 'it', 'suuuuucks!!!', 'WAISTED', '$10...', '#badmovies', '>3:/']\n",
      "\n",
      "WPU :  ['I', 'saw', 'the', 'new', '#', 'johndoe', 'movie', 'and', 'it', 'suuuuucks', '!!!', 'WAISTED', '$', '10', '...', '#', 'badmovies', '>', '3', ':/']\n",
      "\n",
      "SC :  ['I', 'saw', 'the', 'new', '#johndoe', 'movie', 'and', 'it', 'suuuuucks', '!', '!', '!', 'WAISTED', '$10', '.', '.', '.', '#badmovies', '>', '3:/']\n",
      "=====================\n",
      "\n",
      "ORG:  @SentimentSymp:  can't wait for the Nov 9 #Sentiment talks!  YAAAAAAY !!! >:-D http://sentimentsymposium.com/.\n",
      "\n",
      "WSP :  ['@SentimentSymp:', '', \"can't\", 'wait', 'for', 'the', 'Nov', '9', '#Sentiment', 'talks!', '', 'YAAAAAAY', '!!!', '>:-D', 'http://sentimentsymposium.com/.']\n",
      "\n",
      "WPU :  ['@', 'SentimentSymp', ':', 'can', \"'\", 't', 'wait', 'for', 'the', 'Nov', '9', '#', 'Sentiment', 'talks', '!', 'YAAAAAAY', '!!!', '>:-', 'D', 'http', '://', 'sentimentsymposium', '.', 'com', '/.']\n",
      "\n",
      "SC :  ['@SentimentSymp', ':', 'can', \"'\", 't', 'wait', 'for', 'the', 'Nov 9', '#Sentiment', 'talks', '!', 'YAAAAAAY', '!', '!', '!', '>:-D', 'http://sentimentsymposium.com/.']\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "\n",
    "\n",
    "def wsp_tokenizer(text):\n",
    "    return text.split(\" \")\n",
    "\n",
    "puncttok = nltk.WordPunctTokenizer().tokenize\n",
    "\n",
    "social_tokenizer = SocialTokenizer(lowercase=False).tokenize\n",
    "\n",
    "sents = [\n",
    "    \"CANT WAIT for the new season of #TwinPeaks ＼(^o^)／ yaaaay!!! #davidlynch #tvseries :)))\",\n",
    "    \"I saw the new #johndoe movie and it suuuuucks!!! WAISTED $10... #badmovies >3:/\",\n",
    "    \"@SentimentSymp:  can't wait for the Nov 9 #Sentiment talks!  YAAAAAAY !!! >:-D http://sentimentsymposium.com/.\",\n",
    "]\n",
    "\n",
    "for s in sents:\n",
    "    print()\n",
    "    print(\"ORG: \", s)  # original sentence\n",
    "    print()\n",
    "    print(\"WSP : \", wsp_tokenizer(s))  # whitespace tokenizer\n",
    "    print()\n",
    "    print(\"WPU : \", puncttok(s))  # WordPunct tokenizer\n",
    "    print()\n",
    "    print(\"SC : \", social_tokenizer(s))  # social tokenizer\n",
    "    print('=====================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citation\n",
    "\n",
    "@InProceedings{baziotis-pelekis-doulkeridis:2017:SemEval2,\n",
    "  author    = {Baziotis, Christos  and  Pelekis, Nikos  and  Doulkeridis, Christos},\n",
    "  title     = {DataStories at SemEval-2017 Task 4: Deep LSTM with Attention for Message-level and Topic-based Sentiment Analysis},\n",
    "  booktitle = {Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)},\n",
    "  month     = {August},\n",
    "  year      = {2017},\n",
    "  address   = {Vancouver, Canada},\n",
    "  publisher = {Association for Computational Linguistics},\n",
    "  pages     = {747--754}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1] K. Gimpel et al., “Part-of-speech tagging for twitter: Annotation, features, and experiments,” in Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2, 2011, pp. 42–47.\n",
    "\n",
    "[2] C. Potts, “Sentiment Symposium Tutorial: Tokenizing,” Sentiment Symposium Tutorial, 2011. [Online]. Available: http://sentiment.christopherpotts.net/tokenizing.html."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
